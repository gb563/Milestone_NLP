{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Competency-Based GME Assessments: Leveraging Artificial Intelligence to Predict Sub-Competency Content**\n",
        "\n",
        "Development Code\n",
        "\n",
        "Gregory J Booth, MD\n",
        "\n",
        "23 Jan 2022"
      ],
      "metadata": {
        "id": "DUc7M5sHr89Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This digital supplement outlines the code that was used to train an artificial intelligence algorithm to interpret feedback narratives on anesthesiology residents.\n"
      ],
      "metadata": {
        "id": "zCBaXTrcsXVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AhaDeyoeuXwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Install and import pacakges and load data**"
      ],
      "metadata": {
        "id": "deeNZee5ujG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_LLz1KfszMt",
        "outputId": "cd76d62d-7ad3-4ae1-91a4-2544628d0d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3134417 sha256=8e3ea7dad4d893c3ea8077523e6a9a4dd3f10f419921c33cfd6d20d3433fd3e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n"
          ]
        }
      ],
      "source": [
        "#install the package for the AI algorithm\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgVNYUATtLEZ"
      },
      "outputs": [],
      "source": [
        "#import all other necessary packages\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import regex as re\n",
        "from sklearn.preprocessing import label_binarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUlFuILYtd4F"
      },
      "outputs": [],
      "source": [
        "#load files\n",
        "text = pd.read_csv('/content/filename.csv')\n",
        "#text = pd.read_csv('/content/filename_sensitivity.csv') #for sensitivity analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbZmbFDJtgMX"
      },
      "outputs": [],
      "source": [
        "#Combine sub-competencies which had <1% representation. These were identified in phase 1 of the study\n",
        "#where experts labeled each narrative comment.\n",
        "text=text.replace({'PC9':'PC7','PC6':'PC2','ICS1':'ICS','ICS2':'ICS','ICS3':'ICS','P1':'P','P2':'P','P3':'P','PBLI1':'PBLI','PBLI2':'PBLI','SBP1':'SBP','SBP2':'SBP','SBP3':'SBP'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaFqaPRddPOh",
        "outputId": "30b1a553-4610-44f3-d254-ebbb3ec9d853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51428"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#how many sentences are there?\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U8S7TE5V31Lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ueCfhuUkuxmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Separate training and validation cohorts**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pNfMEXqxuza5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwhIUetUgTDV",
        "outputId": "ef8fa28a-6b82-4d9a-efae-84df8547ea85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4510, 4)\n",
            "33922      did a great job on two spine cases  especial...\n",
            "33923      did a great job on two spine cases  especial...\n",
            "33924    Prep  patient knowledge and handling  anesthet...\n",
            "33925    Prep  patient knowledge and handling  anesthet...\n",
            "33926                                  is doing very well \n",
            "Name: Eval, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#separate validation cohort\n",
        "text_test = text[(text['Index'] >=3566) & (text['Index'] <4170)].copy() #pull out Site 3\n",
        "print(text_test.shape)\n",
        "print(text_test['Eval'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvgVt9AkND1L",
        "outputId": "075ac0a5-8f1c-46d0-b607-7322ab0c1310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(46918, 4)\n",
            "0                          Outstanding resident \n",
            "1                          Outstanding resident \n",
            "2                        Very easy to work with \n",
            "3                        Very easy to work with \n",
            "4    came very well prepared  as he always does \n",
            "Name: Eval, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#separate training cohort\n",
        "text_train=text[(text['Index'] <3566) | (text['Index'] >=4170)].copy() #pull out Sites 1, 2, 4\n",
        "print(text_train.shape)\n",
        "print(text_train['Eval'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaWcKG0Kya3F",
        "outputId": "f03df5f2-906c-459f-d37f-f04f4e42f90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11960, 4)\n"
          ]
        }
      ],
      "source": [
        "#process training data\n",
        "#check for comments where both raters agreed\n",
        "retain = []\n",
        "text_test=text_test.reset_index(drop=True)\n",
        "text_train=text_train.reset_index(drop=True)\n",
        "for i in range(len(text_train)-1):\n",
        "  if text_train['Eval'][i]==text_train['Eval'][i+1]:\n",
        "    if text_train['Milestone'][i]==text_train['Milestone'][i+1]:\n",
        "      retain.append(i)\n",
        "\n",
        "len(retain)\n",
        "text_train=text_train.iloc[retain,:]\n",
        "print(text_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af1B46APjKFr",
        "outputId": "74276657-0eee-43f3-e611-e982860c0208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2255, 4)\n"
          ]
        }
      ],
      "source": [
        "#process validation data (full test dataset)\n",
        "text_test=text_test.reset_index(drop=True)\n",
        "retain = []\n",
        "for i in range(len(text_test)-1):\n",
        "  if text_test['Eval'][i]==text_test['Eval'][i+1]:\n",
        "      retain.append(i)\n",
        "\n",
        "len(retain)\n",
        "text_test=text_test.iloc[retain,:]\n",
        "print(text_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#process validation data (sensitivity analysis, just a subset of the data with 100% agreement))\n",
        "retain = [] #process train data\n",
        "text_test=text_test.reset_index(drop=True)\n",
        "for i in range(len(text_test)-1):\n",
        "  if text_test['Eval'][i]==text_test['Eval'][i+1]:\n",
        "    if text_test['Milestone'][i]==text_test['Milestone'][i+1]:\n",
        "      retain.append(i)\n",
        "\n",
        "len(retain)\n",
        "text_test=text_test.iloc[retain,:]\n",
        "print(text_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeERf0yOjHgE",
        "outputId": "0ca6a42e-ebfe-4a7f-8bfe-75b7556f566e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1108, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm18B7FM1v7b"
      },
      "outputs": [],
      "source": [
        "#put data into the form that fastText expects\n",
        "text_train['Milestone']=text_train['Milestone'].apply(lambda x: '__label__'+x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##combine consecutive sentences with same sub-competency in training data\n",
        "text_train=text_train.reset_index(drop=True)\n",
        "combo_text_train_index=[]\n",
        "label_hold=[]\n",
        "new_data2=[]\n",
        "unique=[]\n",
        "index_dup = []\n",
        "orig_index=[]\n",
        "eval_text = ''\n",
        "j=0\n",
        "data_length = len(text_train)\n",
        "while j < (data_length-1):\n",
        "  if text_train['Milestone'][j]==text_train['Milestone'][j+1] and int(text_train['Index'][j])==int(text_train['Index'][j+1]):\n",
        "    #unique.append(text_train['Index'][j])\n",
        "    orig_index.append(text_train['Index'][j])\n",
        "    eval_text=text_train['Eval'][j]\n",
        "    while text_train['Milestone'][j]==text_train['Milestone'][j+1] and int(text_train['Index'][j])==int(text_train['Index'][j+1]):\n",
        "      j+=1\n",
        "      eval_text=eval_text+text_train['Eval'][j]\n",
        "      unique.append(text_train['Index'][j])\n",
        "    label_hold.append((orig_index[-1],text_train['Milestone'][j-1],eval_text))\n",
        "  else:\n",
        "    label_hold.append((text_train['Index'][j],text_train['Milestone'][j],text_train['Eval'][j]))\n",
        "  eval_text=''\n",
        "  j=j+1\n",
        "  if j == data_length-1:\n",
        "    label_hold.append((text_train['Index'][j],text_train['Milestone'][j],text_train['Eval'][j]))\n"
      ],
      "metadata": {
        "id": "x4CJmDmTBn6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build final training dataset\n",
        "\n",
        "evals_final = [i[2] for i in label_hold]\n",
        "milestones_final = [i[1] for i in label_hold]\n",
        "\n",
        "text_train_final = pd.DataFrame(data = None,columns=['Milestone','Eval'])\n",
        "text_train_final['Milestone']=milestones_final\n",
        "text_train_final['Eval']=evals_final"
      ],
      "metadata": {
        "id": "srYKR_EaHr9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train=text_train_final.copy()"
      ],
      "metadata": {
        "id": "QE-SQeB784nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "V7_u97qhvCHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3: Extract descriptive data on training and validation cohorts**"
      ],
      "metadata": {
        "id": "4mHBamdAvDSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#demographic data for train and validation cohorts\n",
        "labels_ordered =['__label__PC1','__label__PC2','__label__PC3','__label__PC4','__label__PC5',\n",
        "                 '__label__PC7','__label__PC8','__label__PC10','__label__MK1','__label__MK2',\n",
        "                 '__label__P','__label__ICS','__label__PBLI','__label__SBP','__label__D','__label__N',]\n",
        "\n",
        "train_dict = Counter(text_train['Milestone'])\n",
        "test_dict = Counter(text_test['Milestone'])\n",
        "print('Category','\\t','Train Count (%)','\\t','Validation Count (%)')\n",
        "train_count=[]\n",
        "test_count=[]\n",
        "for l in labels_ordered:\n",
        "  index = l.strip('__label__')\n",
        "  train_count.append(train_dict[l])\n",
        "  test_count.append(test_dict[index])\n",
        "  print('{:s} \\t {:d} ({:.2f}%) \\t{:d} ({:.2f}%)'.format(index, train_dict[l],100*round(train_dict[l]/len(text_train),3), test_dict[index], 100*round(test_dict[index]/len(text_test),3)))\n",
        "print('total train = ',sum(train_count))\n",
        "print('total test = ',sum(test_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98xbnQAxc2aI",
        "outputId": "2d00a145-34b6-467c-f4ca-2168ce3f0b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category \t Train Count (%) \t Validation Count (%)\n",
            "PC1 \t 369 (3.60%) \t107 (4.70%)\n",
            "PC2 \t 443 (4.30%) \t155 (6.90%)\n",
            "PC3 \t 190 (1.90%) \t49 (2.20%)\n",
            "PC4 \t 601 (5.90%) \t149 (6.60%)\n",
            "PC5 \t 788 (7.70%) \t93 (4.10%)\n",
            "PC7 \t 244 (2.40%) \t81 (3.60%)\n",
            "PC8 \t 85 (0.80%) \t25 (1.10%)\n",
            "PC10 \t 733 (7.20%) \t75 (3.30%)\n",
            "MK1 \t 759 (7.40%) \t172 (7.60%)\n",
            "MK2 \t 187 (1.80%) \t67 (3.00%)\n",
            "P \t 803 (7.90%) \t252 (11.20%)\n",
            "ICS \t 724 (7.10%) \t108 (4.80%)\n",
            "PBLI \t 387 (3.80%) \t156 (6.90%)\n",
            "SBP \t 186 (1.80%) \t76 (3.40%)\n",
            "D \t 1348 (13.20%) \t218 (9.70%)\n",
            "N \t 2371 (23.20%) \t472 (20.90%)\n",
            "total train =  10218\n",
            "total test =  2255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#demographic data for train and validation cohorts after tie-breaking\n",
        "labels_ordered =['__label__PC1','__label__PC2','__label__PC3','__label__PC4','__label__PC5',\n",
        "                 '__label__PC7','__label__PC8','__label__PC10','__label__MK1','__label__MK2',\n",
        "                 '__label__P','__label__ICS','__label__PBLI','__label__SBP','__label__D','__label__N',]\n",
        "\n",
        "train_dict = Counter(text_train['Milestone'])\n",
        "test_dict = Counter(text_test['Milestone'])\n",
        "print('Category','\\t','Train Count (%)','\\t','Validation Count (%)')\n",
        "train_count=[]\n",
        "test_count=[]\n",
        "for l in labels_ordered:\n",
        "  index = l.strip('__label__')\n",
        "  train_count.append(train_dict[l])\n",
        "  test_count.append(test_dict[index])\n",
        "  print('{:s}\\t{:d} ({:.2f}%)'.format(index, test_dict[index], 100*round(test_dict[index]/len(text_test),3)))\n",
        "print('total train = ',sum(train_count))\n",
        "print('total test = ',sum(test_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gV2fvf1sfd0",
        "outputId": "eca49520-6955-4ee6-bdb3-71e44e5873bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category \t Train Count (%) \t Validation Count (%)\n",
            "PC1\t80 (3.50%)\n",
            "PC2\t153 (6.80%)\n",
            "PC3\t58 (2.60%)\n",
            "PC4\t191 (8.50%)\n",
            "PC5\t84 (3.70%)\n",
            "PC7\t90 (4.00%)\n",
            "PC8\t31 (1.40%)\n",
            "PC10\t78 (3.50%)\n",
            "MK1\t158 (7.00%)\n",
            "MK2\t66 (2.90%)\n",
            "P\t297 (13.20%)\n",
            "ICS\t119 (5.30%)\n",
            "PBLI\t149 (6.60%)\n",
            "SBP\t91 (4.00%)\n",
            "D\t222 (9.80%)\n",
            "N\t388 (17.20%)\n",
            "total train =  10218\n",
            "total test =  2255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aMt6VC7ivZUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4: Preprocess training and validation data**"
      ],
      "metadata": {
        "id": "-Y-xgYPjvaSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C49ExRP-hmwl",
        "outputId": "5c1a7109-a086-4396-edda-bb4385db831b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#now let's do some pre-processing to training data\n",
        "#lowercase\n",
        "text_train['Eval']=text_train['Eval'].str.lower()\n",
        "\n",
        "#stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stp = stopwords.words('english')\n",
        "stp.append('throughout')\n",
        "stop_words=stp.copy()\n",
        "def remove_stop(line):\n",
        "  words = nltk.word_tokenize(line)\n",
        "  holder = []\n",
        "  index1=0\n",
        "  for word in words:\n",
        "      if word not in stop_words:\n",
        "          holder.append(word)\n",
        "  return ' '.join(holder)\n",
        "\n",
        "text_train['Eval']=text_train['Eval'].apply(lambda x: remove_stop(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's do some pre-processing to validation data\n",
        "#lowercase\n",
        "text_test['Eval']=text_test['Eval'].str.lower()\n",
        "\n",
        "#stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stp = stopwords.words('english')\n",
        "stp.append('throughout')\n",
        "stop_words=stp.copy()\n",
        "def remove_stop(line):\n",
        "  words = nltk.word_tokenize(line)\n",
        "  holder = []\n",
        "  index1=0\n",
        "  for word in words:\n",
        "      if word not in stop_words:\n",
        "          holder.append(word)\n",
        "  return ' '.join(holder)\n",
        "\n",
        "text_test['Eval']=text_test['Eval'].apply(lambda x: remove_stop(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOFVV7ozeU7X",
        "outputId": "8ead418c-481f-4fa2-c520-de7b122c6cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB_Er1pShIBm"
      },
      "outputs": [],
      "source": [
        "#format training data \n",
        "new = []\n",
        "new=text_train['Milestone']+' '+text_train['Eval']\n",
        "text_train['New']=new"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#final size of training dataset\n",
        "len(new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkctUkRTQFOK",
        "outputId": "7b8e62b3-e3a6-4744-f4b4-3b7169c830ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10218"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cCMfY4tgvyZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 5: Now for the fun part... Find the optimal hyperparameters**"
      ],
      "metadata": {
        "id": "K-Iqwh9MvzZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utZSnfrDlzzo"
      },
      "outputs": [],
      "source": [
        "#set up a grid search\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#shuffle data\n",
        "text_train=text_train.sample(frac=1)\n",
        "text_train=text_train.reset_index(drop=True)\n",
        "\n",
        "results = dict()\n",
        "k_fold_results = []\n",
        "macro_holder=[]\n",
        "epochs = [5, 10, 25, 50]\n",
        "learn_rate = [0.1, 0.25, 0.5, 1]\n",
        "grams=[1, 2, 3]\n",
        "short_long = [(0,0),(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(1,4),(2,4)]\n",
        "\n",
        "for sl in short_long:\n",
        "  for e in epochs:\n",
        "    for l in learn_rate:\n",
        "      for w in grams:\n",
        "        params=str(sl)+', ' +str(e)+ ', ' +str(l)+ ', '+str(w)\n",
        "        kf_macro_f1=[]\n",
        "        kf = KFold(n_splits=5)\n",
        "        for train, test in kf.split(text_train):  \n",
        "          np.savetxt('/content/train_data.txt', text_train['New'][train], fmt='%s')\n",
        "          #np.savetxt('/content/validation_data.txt', text_train[['Milestone','Eval']].iloc[test,:], fmt='%s')\n",
        "          model = fasttext.train_supervised(input='/content/train_data_kf.txt',epoch=e,lr=l,wordNgrams=w,minn=sl[0],maxn=sl[1])\n",
        "          preds = []\n",
        "          for test_index in test:\n",
        "            preds.append(model.predict(text_train['Eval'][test_index])[0])\n",
        "          y_true = text_train['Milestone'][test]\n",
        "          score_macro = f1_score(y_true, preds, average='macro')\n",
        "          macro_holder.append(score_macro)\n",
        "        k_fold_results=sum(macro_holder)/len(macro_holder)\n",
        "        results[params] = k_fold_results\n",
        "        #print(params, ' ',results[params])\n",
        "        k_fold_results=[]\n",
        "        macro_holder=[]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#which hyperparameters gave the highest macro F1 score?\n",
        "sorted(results.items(),key = lambda item: item[1],reverse=True)[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veJ9-5QAQY42",
        "outputId": "0ccd2b05-b838-43de-deb6-75f6ebdcb37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('(1, 4), 25, 1, 2', 0.6835299950606571),\n",
              " ('(1, 4), 25, 1, 1', 0.6801583976137338),\n",
              " ('(1, 4), 50, 1, 3', 0.6798773125341171),\n",
              " ('(1, 4), 25, 1, 3', 0.679850739315856),\n",
              " ('(2, 4), 25, 1, 3', 0.6797712758232317),\n",
              " ('(0, 1), 50, 1, 1', 0.679667121085842),\n",
              " ('(1, 4), 50, 0.5, 3', 0.6795931780412124),\n",
              " ('(2, 4), 50, 0.5, 3', 0.679439256414293),\n",
              " ('(2, 4), 25, 1, 2', 0.6790211359778493),\n",
              " ('(2, 3), 50, 1, 3', 0.6787339201741046)]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uDSq2I1Hv97L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6: Perform internal validation**"
      ],
      "metadata": {
        "id": "7QkP9ITav-t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#maximize function (needed to perform one vs. all analysis)\n",
        "def maximize(line):\n",
        "  big = max(line)\n",
        "  line = [0 if val !=big else 1 for val in line]\n",
        "  return line\n"
      ],
      "metadata": {
        "id": "bu-lewc7Q5if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Stratified bootstrap for internal validation\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "straps = 500\n",
        "labels =['__label__D','__label__ICS','__label__MK1','__label__MK2','__label__N','__label__P',\n",
        "    '__label__PBLI','__label__PC1','__label__PC10','__label__PC2','__label__PC3',\n",
        "    '__label__PC4','__label__PC5','__label__PC7','__label__PC8','__label__SBP']\n",
        "\n",
        "#bootstrap code for internal validity (Fitting model each time)\n",
        "#shuffle training data\n",
        "text_train=text_train.sample(frac=1)\n",
        "text_train=text_train.reset_index(drop=True)\n",
        "\n",
        "#set up class-wise split\n",
        "index_hold = dict()\n",
        "counts = list(Counter(text_train['Milestone']).items())\n",
        "for count in counts:\n",
        "  index_hold[count[0]]=text_train.index[text_train['Milestone']==count[0]].to_list()\n",
        "\n",
        "roc_hold = []\n",
        "roc_auc_dict=defaultdict(list)\n",
        "recall_dict=defaultdict(list)\n",
        "precision_dict=defaultdict(list)\n",
        "F1_dict=defaultdict(list)\n",
        "\n",
        "for strap in range(straps):\n",
        "  #holders for bootstrapped data and results\n",
        "  new_train_index = []\n",
        "  new_test_index = []\n",
        "\n",
        "  for lab in labels:\n",
        "    boot = np.random.choice(index_hold[lab],size=(len(index_hold[lab])),replace=True)\n",
        "    test_index = [x for x in index_hold[lab] if x not in boot]\n",
        "\n",
        "    #now build up entire train and test data\n",
        "    new_train_index.extend(boot)\n",
        "    new_test_index.extend(test_index)\n",
        "\n",
        "  train_data = text_train.iloc[new_train_index]\n",
        "  test_data = text_train.iloc[new_test_index]\n",
        "\n",
        "  np.savetxt('/content/train_data_bootstrap.txt', train_data, fmt='%s')\n",
        "  model = fasttext.train_supervised(input='/content/train_data_bootstrap.txt',epoch=25,lr=1,wordNgrams=2,minn=1,maxn=4)\n",
        "  \n",
        "  preds = []\n",
        "  for t_index in new_test_index:\n",
        "    preds.append(model.predict(test_data['Eval'][t_index])[0])\n",
        "  y_true = test_data['Milestone'][new_test_index]\n",
        "  ##AUC:\n",
        "  y_true_labels=label_binarize(y_true,classes=labels)\n",
        "  y_true_labels_array = np.array(y_true_labels)\n",
        "\n",
        "  pred_update=[]#put float for probability of a true label\n",
        "  preds_dict_array_modified = [] \n",
        "  preds1=[]\n",
        "  for iteration in new_test_index:\n",
        "      i,j = model.predict(test_data['Eval'][iteration],k=16) #probs for each class\n",
        "      preds1.append(i[0])\n",
        "      preds_dict_array_modified.append([1 if cat == i[0] else 0 for cat in labels])\n",
        "\n",
        "  preds_dict_array_modified=np.array(preds_dict_array_modified) \n",
        "  fpr = dict() #finally on to calculating AUC with class vs. rest approach\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  hold = []\n",
        "  for i in range(16):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true_labels_array[:, i], (preds_dict_array_modified[:, i]))\n",
        "      roc_auc[labels[i].strip('__label__')] = auc(fpr[i], tpr[i])\n",
        "      #hold.append(auc(fpr[i], tpr[i]))\n",
        "      \n",
        "  roc_auc_dict[strap]=roc_auc\n",
        "\n",
        "\n",
        "  ##Precision, Recall, F1\n",
        "  result_cv = classification_report(y_true, preds)\n",
        "  #print(result_cv)\n",
        "  processed_cv=result_cv.split()\n",
        "  for i in range(len(processed_cv)):\n",
        "    if processed_cv[i] in labels:\n",
        "      precision_dict[strap].append(np.float(processed_cv[i+1]))\n",
        "      recall_dict[strap].append(np.float(processed_cv[i+2]))\n",
        "      F1_dict[strap].append(np.float(processed_cv[i+3]))\n",
        "\n",
        "  #print(strap, roc_auc)\n",
        "  "
      ],
      "metadata": {
        "id": "ogwP9UAh6Kx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Results for Internal Validation\n",
        "count=0\n",
        "for lab in labels:\n",
        "  add = []\n",
        "  add_pre=[]\n",
        "  add_rec=[]\n",
        "  add_f1=[]\n",
        "  for i in range((500)):\n",
        "    hold1=lab.strip('__label__')\n",
        "    add.append(roc_auc_dict[i][hold1])\n",
        "    add_pre.append(precision_dict[i][count])\n",
        "    add_rec.append(recall_dict[i][count])\n",
        "    add_f1.append(F1_dict[i][count])\n",
        "  mean_roc = sum(add)/straps\n",
        "  mean_prec = sum(add_pre)/straps\n",
        "  mean_rec = sum(add_rec)/straps\n",
        "  mean_f1 = sum(add_f1)/straps\n",
        "  roc_95 = (((sorted(add)[11]+sorted(add)[12])/2),((sorted(add)[488]+sorted(add)[487])/2))\n",
        "  prec_95 =(((sorted(add_pre)[11]+sorted(add_pre)[12])/2),((sorted(add_pre)[488]+sorted(add_pre)[487])/2))\n",
        "  rec_95 = (((sorted(add_rec)[11]+sorted(add_rec)[12])/2),((sorted(add_rec)[488]+sorted(add_rec)[487])/2))\n",
        "  f1_95 = (((sorted(add_f1)[11]+sorted(add_f1)[12])/2),((sorted(add_f1)[488]+sorted(add_f1)[487])/2))\n",
        "  print('{:s}\\t{:.2f} ({:.2f}, {:.2f})\\t{:.2f} ({:.2f}, {:.2f})\\t{:.2f} ({:.2f}, {:.2f}\\t{:.2f} ({:.2f}, {:.2f})'.format(lab.strip('__label__'),mean_prec,prec_95[0],prec_95[1],mean_rec,rec_95[0],rec_95[1],mean_f1,f1_95[0],f1_95[1],mean_roc,roc_95[0],roc_95[1]))"
      ],
      "metadata": {
        "id": "V2asnBzMHp-Y",
        "outputId": "3c849c5f-13cc-4b3f-fecd-90d7fc9f715c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D\t0.78 (0.64, 0.94)\t0.77 (0.55, 0.91)\t0.76 (0.69, 0.82\t0.87 (0.77, 0.92)\n",
            "ICS\t0.76 (0.52, 0.94)\t0.60 (0.34, 0.79)\t0.65 (0.50, 0.73\t0.79 (0.67, 0.87)\n",
            "MK1\t0.83 (0.64, 0.95)\t0.70 (0.51, 0.84)\t0.75 (0.65, 0.81\t0.84 (0.75, 0.91)\n",
            "MK2\t0.51 (0.27, 0.86)\t0.33 (0.14, 0.53)\t0.38 (0.21, 0.51\t0.66 (0.57, 0.76)\n",
            "N\t0.68 (0.53, 0.84)\t0.89 (0.77, 0.97)\t0.76 (0.68, 0.82\t0.87 (0.85, 0.90)\n",
            "P\t0.59 (0.38, 0.82)\t0.63 (0.44, 0.79)\t0.59 (0.50, 0.65\t0.79 (0.71, 0.84)\n",
            "PBLI\t0.59 (0.35, 0.86)\t0.52 (0.31, 0.71)\t0.53 (0.42, 0.62\t0.75 (0.65, 0.84)\n",
            "PC1\t0.72 (0.49, 0.93)\t0.57 (0.33, 0.77)\t0.61 (0.47, 0.71\t0.78 (0.66, 0.87)\n",
            "PC10\t0.88 (0.72, 0.97)\t0.77 (0.54, 0.91)\t0.81 (0.69, 0.87\t0.88 (0.77, 0.94)\n",
            "PC2\t0.58 (0.38, 0.81)\t0.33 (0.08, 0.61)\t0.38 (0.13, 0.54\t0.66 (0.54, 0.79)\n",
            "PC3\t0.72 (0.48, 0.93)\t0.53 (0.29, 0.73)\t0.60 (0.45, 0.71\t0.76 (0.65, 0.86)\n",
            "PC4\t0.58 (0.39, 0.82)\t0.44 (0.11, 0.71)\t0.46 (0.18, 0.57\t0.71 (0.55, 0.83)\n",
            "PC5\t0.88 (0.76, 0.96)\t0.77 (0.56, 0.89)\t0.81 (0.69, 0.86\t0.88 (0.78, 0.94)\n",
            "PC7\t0.44 (0.21, 0.88)\t0.26 (0.05, 0.54)\t0.28 (0.08, 0.42\t0.63 (0.52, 0.76)\n",
            "PC8\t0.59 (0.26, 1.00)\t0.35 (0.11, 0.61)\t0.41 (0.18, 0.62\t0.67 (0.55, 0.80)\n",
            "SBP\t0.31 (0.11, 0.78)\t0.19 (0.01, 0.40)\t0.19 (0.03, 0.31\t0.59 (0.51, 0.69)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save train and test data\n",
        "#train\n",
        "np.savetxt('/content/train_data.txt', text_train['New'], fmt='%s')\n",
        "#test\n",
        "np.savetxt('/content/test_data.txt', text_test['Eval'], fmt='%s')"
      ],
      "metadata": {
        "id": "8jNefIJvu91p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w8vqIx-CwQxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 7: Train the final NLP model!**"
      ],
      "metadata": {
        "id": "6c1o8ix8wRpy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVlx0mznovOr"
      },
      "outputs": [],
      "source": [
        "#Train the final model!!\n",
        "\n",
        "model = fasttext.train_supervised(input='/content/train_data.txt',epoch=25,lr=1,wordNgrams=2,minn=1,maxn=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the data into the format fastText expects\n",
        "text_test['Milestone']=text_test['Milestone'].apply(lambda x: '__label__'+x)"
      ],
      "metadata": {
        "id": "8KDNAOu-0K-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kza7La4swdVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 8: Perform external validation**"
      ],
      "metadata": {
        "id": "FBVcdQy5weZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bootstrap estimates for validation data\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels =['__label__D','__label__ICS','__label__MK1','__label__MK2','__label__N','__label__P',\n",
        "    '__label__PBLI','__label__PC1','__label__PC10','__label__PC2','__label__PC3',\n",
        "    '__label__PC4','__label__PC5','__label__PC7','__label__PC8','__label__SBP']\n",
        "\n",
        "straps = 500\n",
        "text_test=text_test.reset_index(drop=True)\n",
        "#set up class-wise split\n",
        "index_hold = dict()\n",
        "counts = list(Counter(text_test['Milestone']).items())\n",
        "for count in counts:\n",
        "  index_hold[count[0]]=text_test.index[text_test['Milestone']==count[0]].to_list()\n",
        "\n",
        "roc_hold = []\n",
        "roc_auc_dict=defaultdict(list)\n",
        "\n",
        "for strap in range(straps):\n",
        "  #holders for bootstrapped data and results\n",
        "  test1_index = []\n",
        "  test2_index = []\n",
        "\n",
        "  for lab in labels:\n",
        "    boot = np.random.choice(index_hold[lab],size=(len(index_hold[lab])),replace=True)\n",
        "    #now build up entire train and test data\n",
        "    test1_index.extend(boot)\n",
        "\n",
        "  test1_data = text_test.iloc[test1_index]\n",
        "\n",
        "\n",
        "  preds = []\n",
        "\n",
        "  for t in test1_index:\n",
        "      preds.append(model.predict(text_test['Eval'][t])[0])\n",
        "  y_true = text_test['Milestone'].iloc[test1_index]\n",
        "  #y_true = ['__label__'+i for i in y_true]\n",
        "\n",
        "  ##AUC:\n",
        "  y_true_labels=label_binarize(y_true,classes=labels)\n",
        "  y_true_labels_array = np.array(y_true_labels)\n",
        "\n",
        "  pred_update=[]#put float for probability of a true label\n",
        "  preds_dict_array_modified = [] \n",
        "  for iteration in test1_index:\n",
        "      i,j = model.predict(text_test['Eval'][iteration],k=16) #probs for each class\n",
        "      pred_dict = dict()\n",
        "      for count in range(0,16):\n",
        "        pred_dict[i[count]]=j[count]#data=[label for label in j],columns=[col for col in i])\n",
        "\n",
        "      preds1=[]\n",
        "      for lab in labels:\n",
        "        preds1.append(pred_dict[lab])\n",
        "      pred_update.append(preds1) #build our array for all prediction probabilities\n",
        "      \n",
        "  for i in range(len(pred_update)):\n",
        "    preds_dict_array_modified.append(maximize(pred_update[i])) #build prediction array\n",
        "\n",
        "  preds_dict_array_modified=np.array(preds_dict_array_modified) \n",
        "  fpr = dict() #finally on to calculating AUC with class vs. rest approach\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  hold = []\n",
        "  for i in range(16):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true_labels_array[:, i], (preds_dict_array_modified[:, i]))\n",
        "      roc_auc[labels[i].strip('__label__')] = auc(fpr[i], tpr[i])\n",
        "      #hold.append(auc(fpr[i], tpr[i]))\n",
        "      \n",
        "  roc_auc_dict[strap]=roc_auc\n",
        "  #print(roc_auc)"
      ],
      "metadata": {
        "id": "mxps9hIYbgp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Results for validation data\n",
        "for lab in labels_ordered:\n",
        "  add = []\n",
        "  for i in range((500)):\n",
        "    hold1=lab.strip('__label__')\n",
        "    add.append(roc_auc_dict[i][hold1])\n",
        "  mean_roc = sum(add)/straps\n",
        "  roc_95 = (((sorted(add)[11]+sorted(add)[12])/2),((sorted(add)[488]+sorted(add)[487])/2))\n",
        "  print('{:s}\\t{:.2f} ({:.2f}, {:.2f})'.format(lab.strip('__label__'),mean_roc,roc_95[0],roc_95[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q--kJOM-lBc8",
        "outputId": "7604a7ed-fd31-4dc0-81d1-ab4bd039c231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC1\t0.75 (0.69, 0.81)\n",
            "PC2\t0.73 (0.69, 0.77)\n",
            "PC3\t0.71 (0.65, 0.78)\n",
            "PC4\t0.66 (0.63, 0.70)\n",
            "PC5\t0.87 (0.83, 0.92)\n",
            "PC7\t0.64 (0.59, 0.69)\n",
            "PC8\t0.61 (0.55, 0.69)\n",
            "PC10\t0.86 (0.81, 0.90)\n",
            "MK1\t0.85 (0.82, 0.89)\n",
            "MK2\t0.62 (0.57, 0.68)\n",
            "P\t0.62 (0.60, 0.65)\n",
            "ICS\t0.78 (0.74, 0.83)\n",
            "PBLI\t0.70 (0.66, 0.74)\n",
            "SBP\t0.58 (0.54, 0.62)\n",
            "D\t0.82 (0.79, 0.86)\n",
            "N\t0.85 (0.83, 0.88)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bootstrap estimates for validation data, sensitivity analysis (only sentences with 100% agreement)\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels =['__label__D','__label__ICS','__label__MK1','__label__MK2','__label__N','__label__P',\n",
        "    '__label__PBLI','__label__PC1','__label__PC10','__label__PC2','__label__PC3',\n",
        "    '__label__PC4','__label__PC5','__label__PC7','__label__PC8','__label__SBP']\n",
        "\n",
        "straps = 500\n",
        "text_test=text_test.reset_index(drop=True)\n",
        "#set up class-wise split\n",
        "index_hold = dict()\n",
        "counts = list(Counter(text_test['Milestone']).items())\n",
        "for count in counts:\n",
        "  index_hold[count[0]]=text_test.index[text_test['Milestone']==count[0]].to_list()\n",
        "\n",
        "roc_hold = []\n",
        "roc_auc_dict=defaultdict(list)\n",
        "\n",
        "for strap in range(straps):\n",
        "  #holders for bootstrapped data and results\n",
        "  test1_index = []\n",
        "  test2_index = []\n",
        "\n",
        "  for lab in labels:\n",
        "    boot = np.random.choice(index_hold[lab],size=(len(index_hold[lab])),replace=True)\n",
        "    #now build up entire train and test data\n",
        "    test1_index.extend(boot)\n",
        "\n",
        "  test1_data = text_test.iloc[test1_index]\n",
        "\n",
        "\n",
        "  preds = []\n",
        "\n",
        "  for t in test1_index:\n",
        "      preds.append(model.predict(text_test['Eval'][t])[0])\n",
        "  y_true = text_test['Milestone'].iloc[test1_index]\n",
        "  #y_true = ['__label__'+i for i in y_true]\n",
        "\n",
        "  ##AUC:\n",
        "  y_true_labels=label_binarize(y_true,classes=labels)\n",
        "  y_true_labels_array = np.array(y_true_labels)\n",
        "\n",
        "  pred_update=[]#put float for probability of a true label\n",
        "  preds_dict_array_modified = [] \n",
        "  for iteration in test1_index:\n",
        "      i,j = model.predict(text_test['Eval'][iteration],k=16) #probs for each class\n",
        "      pred_dict = dict()\n",
        "      for count in range(0,16):\n",
        "        pred_dict[i[count]]=j[count]\n",
        "\n",
        "      preds1=[]\n",
        "      for lab in labels:\n",
        "        preds1.append(pred_dict[lab])\n",
        "      pred_update.append(preds1) #build our array for all prediction probabilities\n",
        "      \n",
        "  for i in range(len(pred_update)):\n",
        "    preds_dict_array_modified.append(maximize(pred_update[i])) #build prediction array\n",
        "\n",
        "  preds_dict_array_modified=np.array(preds_dict_array_modified) \n",
        "  fpr = dict() #finally on to calculating AUC with class vs. rest approach\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  hold = []\n",
        "  for i in range(16):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_true_labels_array[:, i], (preds_dict_array_modified[:, i]))\n",
        "      roc_auc[labels[i].strip('__label__')] = auc(fpr[i], tpr[i])\n",
        "      #hold.append(auc(fpr[i], tpr[i]))\n",
        "      \n",
        "  roc_auc_dict[strap]=roc_auc\n",
        "  #print(roc_auc)"
      ],
      "metadata": {
        "id": "AWOIiheZqZBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Results for validation data, sensitivity analysis (total agreement between raters on validation data)\n",
        "for lab in labels_ordered:\n",
        "  add = []\n",
        "  for i in range((500)):\n",
        "    hold1=lab.strip('__label__')\n",
        "    add.append(roc_auc_dict[i][hold1])\n",
        "  mean_roc = sum(add)/straps\n",
        "  roc_95 = (((sorted(add)[11]+sorted(add)[12])/2),((sorted(add)[488]+sorted(add)[487])/2))\n",
        "  print('{:s}\\t{:.2f} ({:.2f}, {:.2f})'.format(lab.strip('__label__'),mean_roc,roc_95[0],roc_95[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAAlyMhf7UXH",
        "outputId": "73de6a61-269f-47f3-c69f-3ba4d53712b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC1\t0.83 (0.76, 0.89)\n",
            "PC2\t0.75 (0.69, 0.82)\n",
            "PC3\t0.76 (0.65, 0.87)\n",
            "PC4\t0.75 (0.69, 0.81)\n",
            "PC5\t0.90 (0.86, 0.95)\n",
            "PC7\t0.72 (0.62, 0.81)\n",
            "PC8\t0.63 (0.53, 0.73)\n",
            "PC10\t0.87 (0.80, 0.93)\n",
            "MK1\t0.92 (0.89, 0.95)\n",
            "MK2\t0.65 (0.56, 0.78)\n",
            "P\t0.73 (0.67, 0.78)\n",
            "ICS\t0.87 (0.81, 0.92)\n",
            "PBLI\t0.80 (0.74, 0.87)\n",
            "SBP\t0.62 (0.50, 0.73)\n",
            "D\t0.88 (0.85, 0.91)\n",
            "N\t0.91 (0.89, 0.93)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}